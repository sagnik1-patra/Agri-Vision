{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d215a38c-3a53-4852-aca4-6f9d0cba7284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 13:32:53,281 | INFO | Reading: C:\\Users\\sagni\\Downloads\\Agri Vision\\archive\\pesticides.csv\n",
      "2025-08-29 13:32:53,511 | INFO | Reading: C:\\Users\\sagni\\Downloads\\Agri Vision\\archive\\rainfall.csv\n",
      "2025-08-29 13:32:53,533 | INFO | Reading: C:\\Users\\sagni\\Downloads\\Agri Vision\\archive\\temp.csv\n",
      "2025-08-29 13:32:53,725 | INFO | Reading: C:\\Users\\sagni\\Downloads\\Agri Vision\\archive\\yield.csv\n",
      "2025-08-29 13:32:54,155 | INFO | Reading: C:\\Users\\sagni\\Downloads\\Agri Vision\\archive\\yield_df.csv\n",
      "2025-08-29 13:32:54,368 | INFO | Anchor selected: yield_df.csv (direct)\n",
      "2025-08-29 13:32:54,376 | INFO | Anchor ready with yield. Shape: (28242, 3)\n",
      "2025-08-29 13:32:54,377 | WARNING | [pest] Not enough common keys to safely join (found: ['year']). Skipping.\n",
      "2025-08-29 13:32:54,378 | WARNING | [rain] Not enough common keys to safely join (found: ['year']). Skipping.\n",
      "2025-08-29 13:32:54,378 | WARNING | [temp] Not enough common keys to safely join (found: ['year']). Skipping.\n",
      "2025-08-29 13:32:54,411 | INFO | [SKLEARN] Fitting...\n",
      "2025-08-29 13:32:59,094 | INFO | [SKLEARN] Saved → C:\\Users\\sagni\\Downloads\\Agri Vision\\yield_sklearn_pipeline.pkl\n",
      "2025-08-29 13:32:59,097 | INFO | [PREPROC] Saved → C:\\Users\\sagni\\Downloads\\Agri Vision\\preprocessor_only.pkl\n",
      "2025-08-29 13:32:59,421 | INFO | [KERAS] Training...\n",
      "2025-08-29 13:33:41,122 | WARNING | You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-08-29 13:33:41,144 | INFO | [KERAS] Saved → C:\\Users\\sagni\\Downloads\\Agri Vision\\yield_mlp.h5\n",
      "2025-08-29 13:33:41,841 | INFO | Saved plots:\n",
      "- C:\\Users\\sagni\\Downloads\\Agri Vision\\accuracy_curve.png\n",
      "- C:\\Users\\sagni\\Downloads\\Agri Vision\\corr_heatmap.png\n",
      "2025-08-29 13:33:41,842 | INFO | === AgriVision ML Artifacts Ready ===\n",
      "2025-08-29 13:33:41,843 | INFO | PKL -> C:\\Users\\sagni\\Downloads\\Agri Vision\\yield_sklearn_pipeline.pkl\n",
      "2025-08-29 13:33:41,843 | INFO | H5  -> C:\\Users\\sagni\\Downloads\\Agri Vision\\yield_mlp.h5\n",
      "2025-08-29 13:33:41,843 | INFO | YAML-> C:\\Users\\sagni\\Downloads\\Agri Vision\\agrivision_config.yaml\n",
      "2025-08-29 13:33:41,844 | INFO | JSON-> C:\\Users\\sagni\\Downloads\\Agri Vision\\metrics.json\n",
      "2025-08-29 13:33:41,844 | INFO | CSV -> C:\\Users\\sagni\\Downloads\\Agri Vision\\sample_predictions.csv\n",
      "2025-08-29 13:33:41,844 | INFO | HISTORY -> C:\\Users\\sagni\\Downloads\\Agri Vision\\history.csv\n",
      "2025-08-29 13:33:41,845 | INFO | TRAIN_FRAME -> C:\\Users\\sagni\\Downloads\\Agri Vision\\training_frame.csv\n",
      "2025-08-29 13:33:41,846 | INFO | ACCURACY_PNG -> C:\\Users\\sagni\\Downloads\\Agri Vision\\accuracy_curve.png\n",
      "2025-08-29 13:33:41,846 | INFO | HEATMAP_PNG  -> C:\\Users\\sagni\\Downloads\\Agri Vision\\corr_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, yaml, logging, warnings\n",
    "from typing import List, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Quiet TF/Keras logs\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ------------------------\n",
    "# User Paths (RAW STRINGS)\n",
    "# ------------------------\n",
    "CSV_PESTICIDES = r\"C:\\Users\\sagni\\Downloads\\Agri Vision\\archive\\pesticides.csv\"\n",
    "CSV_RAINFALL   = r\"C:\\Users\\sagni\\Downloads\\Agri Vision\\archive\\rainfall.csv\"\n",
    "CSV_TEMP       = r\"C:\\Users\\sagni\\Downloads\\Agri Vision\\archive\\temp.csv\"\n",
    "CSV_YIELD      = r\"C:\\Users\\sagni\\Downloads\\Agri Vision\\archive\\yield.csv\"\n",
    "CSV_YIELD_DF   = r\"C:\\Users\\sagni\\Downloads\\Agri Vision\\archive\\yield_df.csv\"\n",
    "\n",
    "ARTIFACT_DIR   = r\"C:\\Users\\sagni\\Downloads\\Agri Vision\"\n",
    "PKL_PATH       = os.path.join(ARTIFACT_DIR, \"yield_sklearn_pipeline.pkl\")\n",
    "H5_PATH        = os.path.join(ARTIFACT_DIR, \"yield_mlp.h5\")\n",
    "CONFIG_YAML    = os.path.join(ARTIFACT_DIR, \"agrivision_config.yaml\")\n",
    "METRICS_JSON   = os.path.join(ARTIFACT_DIR, \"metrics.json\")\n",
    "PRED_SAMPLE    = os.path.join(ARTIFACT_DIR, \"sample_predictions.csv\")\n",
    "PREPROC_PKL    = os.path.join(ARTIFACT_DIR, \"preprocessor_only.pkl\")\n",
    "HISTORY_CSV    = os.path.join(ARTIFACT_DIR, \"history.csv\")\n",
    "TRAIN_FRAME_CSV= os.path.join(ARTIFACT_DIR, \"training_frame.csv\")\n",
    "ACC_PNG        = os.path.join(ARTIFACT_DIR, \"accuracy_curve.png\")\n",
    "HEATMAP_PNG    = os.path.join(ARTIFACT_DIR, \"corr_heatmap.png\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "POSSIBLE_KEYS = [\"year\",\"state\",\"district\",\"crop\",\"season\",\"month\",\"region\",\"block\"]\n",
    "\n",
    "# ------------------------\n",
    "# Utils\n",
    "# ------------------------\n",
    "def ensure_dir(path: str): os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [str(c).strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def downcast_memory(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.select_dtypes(include=[\"int\",\"float\",\"int64\",\"float64\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\",\n",
    "                              downcast=\"integer\" if \"int\" in str(df[c].dtype) else \"float\")\n",
    "    for c in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        uniq = df[c].nunique(dropna=True)\n",
    "        if uniq and uniq <= max(50, df.shape[0]//10):\n",
    "            df[c] = df[c].astype(\"category\")\n",
    "    return df\n",
    "\n",
    "def safe_read_csv(path: str) -> pd.DataFrame:\n",
    "    logging.info(f\"Reading: {path}\")\n",
    "    if not os.path.exists(path):\n",
    "        logging.warning(f\"Missing: {path}\")\n",
    "        return pd.DataFrame()\n",
    "    try:\n",
    "        df = pd.read_csv(path, engine=\"python\", memory_map=True)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Read failed ({e}); retrying with latin-1\")\n",
    "        df = pd.read_csv(path, engine=\"python\", encoding=\"latin-1\")\n",
    "    return downcast_memory(normalize_cols(df))\n",
    "\n",
    "def canonicalize(name: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"_\", str(name).lower()).strip(\"_\")\n",
    "\n",
    "def find_first(df: pd.DataFrame, patterns: List[str]) -> Optional[str]:\n",
    "    canon_cols = {c: canonicalize(c) for c in df.columns}\n",
    "    for pat in patterns:\n",
    "        rx = re.compile(pat)\n",
    "        for col, can in canon_cols.items():\n",
    "            if rx.fullmatch(can) or rx.search(can):\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "def get_keys_in_common(df1: pd.DataFrame, df2: pd.DataFrame) -> List[str]:\n",
    "    inter = [k for k in POSSIBLE_KEYS if (k in df1.columns and k in df2.columns)]\n",
    "    for pref in [\n",
    "        [\"year\",\"state\",\"district\",\"crop\"],\n",
    "        [\"year\",\"state\",\"crop\"],\n",
    "        [\"year\",\"state\",\"district\"],\n",
    "        [\"year\",\"state\"],\n",
    "        [\"year\",\"region\",\"crop\"],\n",
    "        [\"year\",\"region\"],\n",
    "    ]:\n",
    "        if set(pref).issubset(inter): return pref\n",
    "    if len(inter) >= 2: return inter[:3]\n",
    "    if \"year\" in inter:\n",
    "        more = [k for k in inter if k != \"year\"]\n",
    "        if more: return [\"year\", more[0]]\n",
    "    return inter\n",
    "\n",
    "def aggregate_by_keys(df: pd.DataFrame, keys: List[str], feature_prefix: str) -> pd.DataFrame:\n",
    "    if df.empty or not keys: return pd.DataFrame()\n",
    "    num_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    num_cols = [c for c in num_cols if c not in keys]\n",
    "    if not num_cols:\n",
    "        g = df.groupby(keys, dropna=False).size().reset_index(name=f\"{feature_prefix}_count\")\n",
    "        return g\n",
    "    agg = df.groupby(keys, dropna=False)[num_cols].agg([\"mean\",\"sum\",\"max\",\"min\"]).reset_index()\n",
    "    agg.columns = [\"_\".join([c for c in col if c]).strip(\"_\") for col in agg.columns.to_flat_index()]\n",
    "    for c in agg.columns:\n",
    "        if c not in keys:\n",
    "            agg.rename(columns={c: f\"{feature_prefix}__{c}\"}, inplace=True)\n",
    "    return agg\n",
    "\n",
    "# ------------------------\n",
    "# Build a unified anchor with guaranteed 'yield'\n",
    "# ------------------------\n",
    "def make_unified_yield(df_y: pd.DataFrame, df_y2: pd.DataFrame) -> pd.DataFrame:\n",
    "    cand_tables = []\n",
    "    for name, d in [(\"yield.csv\", df_y), (\"yield_df.csv\", df_y2)]:\n",
    "        if d.empty: \n",
    "            continue\n",
    "        d = d.drop_duplicates().copy()\n",
    "        y_col = find_first(d, [\n",
    "            r\"^yield(_.*)?$\", r\".*yield.*\", r\"yield_t_ha\", r\"yield_q_ha\",\n",
    "            r\"yield_kg_per_ha\", r\"yield_tonnes_per_hectare\", r\"yield_per_ha\", r\"yield_df\"\n",
    "        ])\n",
    "        if y_col:\n",
    "            d[\"yield\"] = pd.to_numeric(d[y_col], errors=\"coerce\")\n",
    "            cand_tables.append((\"direct\", name, d))\n",
    "            continue\n",
    "        prod_col = find_first(d, [r\"^production(_.*)?$\", r\".*production.*\"])\n",
    "        area_col = find_first(d,  [r\"^area(_.*)?$\", r\".*area.*\", r\".*hectare.*\"])\n",
    "        if prod_col and area_col:\n",
    "            with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "                d[\"yield\"] = pd.to_numeric(d[prod_col], errors=\"coerce\") / \\\n",
    "                             pd.to_numeric(d[area_col], errors=\"coerce\").replace(0, np.nan)\n",
    "            cand_tables.append((\"computed\", name, d))\n",
    "        else:\n",
    "            cand_tables.append((\"no_yet\", name, d))\n",
    "\n",
    "    usable = [(k, n, d) for (k, n, d) in cand_tables if \"yield\" in d.columns]\n",
    "    if usable:\n",
    "        def key_score(d):\n",
    "            return sum(1 for k in [\"year\",\"state\",\"district\",\"crop\",\"season\",\"region\"] if k in d.columns)\n",
    "        usable.sort(key=lambda x: key_score(x[2]), reverse=True)\n",
    "        logging.info(f\"Anchor selected: {usable[0][1]} ({usable[0][0]})\")\n",
    "        return usable[0][2]\n",
    "\n",
    "    if len(cand_tables) == 2:\n",
    "        (k1, n1, d1), (k2, n2, d2) = cand_tables\n",
    "        keys = get_keys_in_common(d1, d2)\n",
    "        if len(keys) >= 2:\n",
    "            merged = pd.merge(d1, d2, how=\"inner\", on=keys, suffixes=(\"_1\",\"_2\"))\n",
    "            prod_col = find_first(merged, [r\"^production(_.*)?$\", r\".*production.*\"])\n",
    "            area_col = find_first(merged,  [r\"^area(_.*)?$\", r\".*area.*\", r\".*hectare.*\"])\n",
    "            if prod_col and area_col:\n",
    "                with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "                    merged[\"yield\"] = pd.to_numeric(merged[prod_col], errors=\"coerce\") / \\\n",
    "                                      pd.to_numeric(merged[area_col], errors=\"coerce\").replace(0, np.nan)\n",
    "                logging.info(f\"Anchor constructed by merge of {n1} + {n2} on keys={keys}\")\n",
    "                return merged\n",
    "\n",
    "    raise ValueError(\"Could not find or compute 'yield' from either yield.csv or yield_df.csv.\")\n",
    "\n",
    "# ------------------------\n",
    "# Modeling helpers\n",
    "# ------------------------\n",
    "def build_sklearn_pipeline(X: pd.DataFrame) -> Pipeline:\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype.name in (\"object\",\"category\")]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    # Handle scikit-learn version differences\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)  # >=1.4\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)         # <=1.3\n",
    "\n",
    "    transformers = []\n",
    "    if num_cols:\n",
    "        transformers.append((\"num\", StandardScaler(with_mean=False), num_cols))\n",
    "    if cat_cols:\n",
    "        transformers.append((\"cat\", ohe, cat_cols))\n",
    "\n",
    "    pre = ColumnTransformer(transformers=transformers, remainder=\"drop\", sparse_threshold=0.3)\n",
    "    rf = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "    return Pipeline([(\"pre\", pre), (\"rf\", rf)])\n",
    "\n",
    "def keras_mlp(input_dim: int) -> keras.Model:\n",
    "    inp = keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(256, activation=\"relu\")(inp); x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x);   x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    out = layers.Dense(1, activation=\"linear\")(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
    "                  loss=\"mse\",\n",
    "                  metrics=[keras.metrics.RootMeanSquaredError(name=\"rmse\"),\n",
    "                           keras.metrics.MeanAbsoluteError(name=\"mae\")])\n",
    "    return model\n",
    "\n",
    "# ------------------------\n",
    "# Plotting\n",
    "# ------------------------\n",
    "def plot_accuracy(history_csv: str, out_png: str):\n",
    "    hist = pd.read_csv(history_csv)\n",
    "    rmse = \"rmse\" if \"rmse\" in hist.columns else (\"root_mean_squared_error\" if \"root_mean_squared_error\" in hist.columns else None)\n",
    "    val_rmse = \"val_rmse\" if \"val_rmse\" in hist.columns else (\"val_root_mean_squared_error\" if \"val_root_mean_squared_error\" in hist.columns else None)\n",
    "    mae = \"mae\" if \"mae\" in hist.columns else None\n",
    "    val_mae = \"val_mae\" if \"val_mae\" in hist.columns else None\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    if rmse and val_rmse:\n",
    "        plt.plot(hist[\"epoch\"], hist[rmse], label=\"Train RMSE\")\n",
    "        plt.plot(hist[\"epoch\"], hist[val_rmse], label=\"Val RMSE\")\n",
    "    if mae and val_mae:\n",
    "        plt.plot(hist[\"epoch\"], hist[mae], label=\"Train MAE\")\n",
    "        plt.plot(hist[\"epoch\"], hist[val_mae], label=\"Val MAE\")\n",
    "    if (\"loss\" in hist.columns) and (\"val_loss\" in hist.columns):\n",
    "        plt.plot(hist[\"epoch\"], np.sqrt(hist[\"loss\"]), label=\"Train RMSE (from loss)\")\n",
    "        plt.plot(hist[\"epoch\"], np.sqrt(hist[\"val_loss\"]), label=\"Val RMSE (from loss)\")\n",
    "    plt.title(\"Training Curves (RMSE / MAE)\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Metric\")\n",
    "    plt.legend(); plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "    plt.tight_layout(); plt.savefig(out_png, dpi=160); plt.close()\n",
    "\n",
    "def plot_corr_heatmap(train_frame_csv: str, out_png: str):\n",
    "    df = pd.read_csv(train_frame_csv)\n",
    "    num_df = df.select_dtypes(include=[\"number\"]).copy()\n",
    "    if num_df.shape[1] == 0:\n",
    "        raise RuntimeError(\"No numeric columns found for correlation heatmap.\")\n",
    "    corr = num_df.corr(numeric_only=True)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    im = plt.imshow(corr.values, interpolation=\"nearest\", aspect=\"auto\")\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    labels = list(corr.columns)\n",
    "    plt.title(\"Feature Correlation Heatmap\")\n",
    "    plt.xticks(range(len(labels)), labels, rotation=90)\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    plt.tight_layout(); plt.savefig(out_png, dpi=160); plt.close()\n",
    "\n",
    "# ------------------------\n",
    "# Main (train + plot)\n",
    "# ------------------------\n",
    "def main():\n",
    "    ensure_dir(ARTIFACT_DIR)\n",
    "\n",
    "    # Read all\n",
    "    df_pest = safe_read_csv(CSV_PESTICIDES)\n",
    "    df_rain = safe_read_csv(CSV_RAINFALL)\n",
    "    df_temp = safe_read_csv(CSV_TEMP)\n",
    "    df_y    = safe_read_csv(CSV_YIELD)\n",
    "    df_y2   = safe_read_csv(CSV_YIELD_DF)\n",
    "\n",
    "    # Build anchor with guaranteed 'yield'\n",
    "    anchor = make_unified_yield(df_y, df_y2)\n",
    "    anchor = normalize_cols(anchor).drop_duplicates()\n",
    "\n",
    "    key_cols = [k for k in POSSIBLE_KEYS if k in anchor.columns]\n",
    "    keep = list(dict.fromkeys(key_cols + [\"yield\",\"production\",\"area\",\"crop\",\"state\",\"district\",\"season\",\"year\",\"region\",\"block\"]))\n",
    "    anchor = anchor[[c for c in keep if c in anchor.columns]].copy()\n",
    "    logging.info(f\"Anchor ready with yield. Shape: {anchor.shape}\")\n",
    "\n",
    "    # Aggregate + left-join side tables (require >=2 keys to avoid blowups)\n",
    "    for prefix, tbl in [(\"pest\", df_pest), (\"rain\", df_rain), (\"temp\", df_temp)]:\n",
    "        if tbl.empty:\n",
    "            logging.info(f\"Skipping empty table: {prefix}\")\n",
    "            continue\n",
    "        keys = get_keys_in_common(anchor, tbl)\n",
    "        if len(keys) < 2:\n",
    "            logging.warning(f\"[{prefix}] Not enough common keys to safely join (found: {keys}). Skipping.\")\n",
    "            continue\n",
    "        agg = aggregate_by_keys(tbl, keys, prefix)\n",
    "        if agg.empty:\n",
    "            logging.warning(f\"[{prefix}] Aggregation yielded empty table. Skipping.\")\n",
    "            continue\n",
    "        anchor = pd.merge(anchor, downcast_memory(agg), how=\"left\", on=keys)\n",
    "        logging.info(f\"[{prefix}] Joined. Anchor shape: {anchor.shape}\")\n",
    "\n",
    "    # Final clean\n",
    "    for c in anchor.columns:\n",
    "        if anchor[c].dtype.name in (\"object\",\"string\"):\n",
    "            anchor[c] = anchor[c].astype(str).str.strip()\n",
    "    anchor = anchor.dropna(subset=[\"yield\"])\n",
    "    anchor = downcast_memory(anchor)\n",
    "\n",
    "    # Save full frame for plotting\n",
    "    anchor.to_csv(TRAIN_FRAME_CSV, index=False)\n",
    "\n",
    "    # Split\n",
    "    y = anchor[\"yield\"].astype(float)\n",
    "    X = anchor.drop(columns=[\"yield\"], errors=\"ignore\")\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 1) Sklearn pipeline\n",
    "    pipe = build_sklearn_pipeline(X_train)\n",
    "    logging.info(\"[SKLEARN] Fitting...\")\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred_val = pipe.predict(X_val)\n",
    "    rf_metrics = {\n",
    "        \"r2\": float(r2_score(y_val, y_pred_val)),\n",
    "        \"mae\": float(mean_absolute_error(y_val, y_pred_val)),\n",
    "        \"rmse\": float(np.sqrt(mean_squared_error(y_val, y_pred_val))),\n",
    "    }\n",
    "    joblib.dump(pipe, PKL_PATH); logging.info(f\"[SKLEARN] Saved → {PKL_PATH}\")\n",
    "\n",
    "    # Save preprocessor for Keras\n",
    "    pre = pipe.named_steps[\"pre\"]; joblib.dump(pre, PREPROC_PKL); logging.info(f\"[PREPROC] Saved → {PREPROC_PKL}\")\n",
    "\n",
    "    # 2) Keras MLP\n",
    "    Xt_train = pre.transform(X_train); Xt_val = pre.transform(X_val)\n",
    "    if hasattr(Xt_train, \"toarray\"): Xt_train = Xt_train.toarray()\n",
    "    if hasattr(Xt_val, \"toarray\"):   Xt_val   = Xt_val.toarray()\n",
    "\n",
    "    model = keras_mlp(Xt_train.shape[1])\n",
    "    es = keras.callbacks.EarlyStopping(monitor=\"val_rmse\", patience=10, restore_best_weights=True)\n",
    "    logging.info(\"[KERAS] Training...\")\n",
    "    hist = model.fit(Xt_train, y_train.values.astype(\"float32\"),\n",
    "                     validation_data=(Xt_val, y_val.values.astype(\"float32\")),\n",
    "                     epochs=200, batch_size=256, verbose=0, callbacks=[es])\n",
    "\n",
    "    # Save history for plotting\n",
    "    hist_df = pd.DataFrame(hist.history)\n",
    "    rename_map = {\"root_mean_squared_error\":\"rmse\", \"val_root_mean_squared_error\":\"val_rmse\"}\n",
    "    hist_df.rename(columns=rename_map, inplace=True)\n",
    "    hist_df.insert(0, \"epoch\", np.arange(1, len(hist_df)+1))\n",
    "    hist_df.to_csv(HISTORY_CSV, index=False)\n",
    "\n",
    "    y_pred_val_mlp = model.predict(Xt_val, verbose=0).ravel()\n",
    "    mlp_metrics = {\n",
    "        \"r2\": float(r2_score(y_val, y_pred_val_mlp)),\n",
    "        \"mae\": float(mean_absolute_error(y_val, y_pred_val_mlp)),\n",
    "        \"rmse\": float(np.sqrt(mean_squared_error(y_val, y_pred_val_mlp))),\n",
    "        \"epochs_trained\": int(len(hist.history[\"loss\"]))\n",
    "    }\n",
    "    model.save(H5_PATH); logging.info(f\"[KERAS] Saved → {H5_PATH}\")\n",
    "\n",
    "    # Meta\n",
    "    metrics_all = {\n",
    "        \"rows_final\": int(anchor.shape[0]),\n",
    "        \"features_final\": int(X.shape[1]),\n",
    "        \"sklearn_random_forest\": rf_metrics,\n",
    "        \"keras_mlp\": mlp_metrics\n",
    "    }\n",
    "    with open(METRICS_JSON, \"w\", encoding=\"utf-8\") as f: json.dump(metrics_all, f, indent=2)\n",
    "\n",
    "    cfg = {\n",
    "        \"data_paths\": {\n",
    "            \"pesticides\": CSV_PESTICIDES,\n",
    "            \"rainfall\": CSV_RAINFALL,\n",
    "            \"temp\": CSV_TEMP,\n",
    "            \"yield\": CSV_YIELD,\n",
    "            \"yield_df\": CSV_YIELD_DF\n",
    "        },\n",
    "        \"artifact_dir\": ARTIFACT_DIR,\n",
    "        \"target\": \"yield\",\n",
    "        \"models\": {\"sklearn_pipeline_pkl\": os.path.basename(PKL_PATH), \"keras_model_h5\": os.path.basename(H5_PATH)},\n",
    "        \"preprocessor\": os.path.basename(PREPROC_PKL)\n",
    "    }\n",
    "    with open(CONFIG_YAML, \"w\", encoding=\"utf-8\") as f: yaml.safe_dump(cfg, f, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "    sample = X_val.copy()\n",
    "    sample[\"y_true\"] = y_val\n",
    "    sample[\"y_pred_rf\"] = y_pred_val\n",
    "    sample[\"y_pred_mlp\"] = y_pred_val_mlp\n",
    "    sample.head(200).to_csv(PRED_SAMPLE, index=False)\n",
    "\n",
    "    # === PLOTS (immediately after training) ===\n",
    "    try:\n",
    "        plot_accuracy(HISTORY_CSV, ACC_PNG)\n",
    "        plot_corr_heatmap(TRAIN_FRAME_CSV, HEATMAP_PNG)\n",
    "        logging.info(f\"Saved plots:\\n- {ACC_PNG}\\n- {HEATMAP_PNG}\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Plotting skipped due to: {e}\")\n",
    "\n",
    "    logging.info(\"=== AgriVision ML Artifacts Ready ===\")\n",
    "    logging.info(f\"PKL -> {PKL_PATH}\")\n",
    "    logging.info(f\"H5  -> {H5_PATH}\")\n",
    "    logging.info(f\"YAML-> {CONFIG_YAML}\")\n",
    "    logging.info(f\"JSON-> {METRICS_JSON}\")\n",
    "    logging.info(f\"CSV -> {PRED_SAMPLE}\")\n",
    "    logging.info(f\"HISTORY -> {HISTORY_CSV}\")\n",
    "    logging.info(f\"TRAIN_FRAME -> {TRAIN_FRAME_CSV}\")\n",
    "    logging.info(f\"ACCURACY_PNG -> {ACC_PNG}\")\n",
    "    logging.info(f\"HEATMAP_PNG  -> {HEATMAP_PNG}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce81a86e-8c2f-4787-a446-a553731a41f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
